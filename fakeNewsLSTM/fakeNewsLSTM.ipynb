{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fakeNews.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPxaPUegl29IU2h6NKPuJmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nakdimon/fakeNewsClassifier/blob/main/fakeNewsLSTM/fakeNewsLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BONy3j7s-vD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "c0bEI6n6tRGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourceFake = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/fake-real-news-dataset/main/data/Fake.csv')\n",
        "sourceReal = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/fake-real-news-dataset/main/data/True.csv')\n",
        "sourceTrain = pd.read_csv('https://raw.githubusercontent.com/Nakdimon/fakeNewsClassifier/main/data/train.csv')\n",
        "sourceTest = pd.read_csv('https://raw.githubusercontent.com/Nakdimon/fakeNewsClassifier/main/data/test.csv')"
      ],
      "metadata": {
        "id": "5i9htzE6tRHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = sourceFake\n",
        "real = sourceReal"
      ],
      "metadata": {
        "id": "mCiMgJ8qt5Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = sourceTrain\n",
        "test = sourceTest"
      ],
      "metadata": {
        "id": "B7Y-2NBQlaOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.rename(columns = {'Labels':'label', 'Text':'text'})\n",
        "labelCol = train.pop('label')\n",
        "train = train.join(labelCol)"
      ],
      "metadata": {
        "id": "HSrWyJC808xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing clusters with binary labels\n",
        "train['label'] = train['label'].replace({0:0, 1:0, 2:0, 3:1, 4:1, 5:1})"
      ],
      "metadata": {
        "id": "wt-u68Ef2rky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_real = train[train['label']==1]\n",
        "train_fake = train[train['label']==0]"
      ],
      "metadata": {
        "id": "vADCRvJyk3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming columns to match other dataframes\n",
        "test = test.rename(columns = {'Text':'text'})"
      ],
      "metadata": {
        "id": "vc8iqwl00_FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake['subject'].value_counts()"
      ],
      "metadata": {
        "id": "qnaJs6uFz3cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "sns.countplot(x = 'subject', data = fake)"
      ],
      "metadata": {
        "id": "r3gO0JpWz-RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(fake['text'].tolist())\n",
        "text2 = ' '.join(train_real['text'].tolist())\n",
        "text5 = ' '.join(train_fake['text'].tolist())\n",
        "text3 = ' '.join(test['text'].tolist())\n",
        "text4 = ' '.join(real['text'].tolist())"
      ],
      "metadata": {
        "id": "6sZovd9J0Z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing word presence in fake dataset\n",
        "wordcloud = WordCloud().generate(text)\n",
        "fig = plt.figure(figsize = (10,20))\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "6W3kNZAu0fML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing word presence in fake dataset\n",
        "wordcloud = WordCloud().generate(text4)\n",
        "fig = plt.figure(figsize = (10,20))\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "V-kMhqGj5INx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing word presence in train dataset\n",
        "wordcloud = WordCloud().generate(text2)\n",
        "fig = plt.figure(figsize = (10,20))\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "1cypoHWU4VKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing word presence in train dataset\n",
        "wordcloud = WordCloud().generate(text5)\n",
        "fig = plt.figure(figsize = (10,20))\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "VRLT8rULmVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing word presence in test dataset\n",
        "wordcloud = WordCloud().generate(text3)\n",
        "fig = plt.figure(figsize = (10,20))\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "5JTIZLNe4U9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for unknown_publishers on the real news dataset to fill in with Unknown tag5\n",
        "unknown_publishers = []\n",
        "for index, row in enumerate(real.text.values):\n",
        "  try:\n",
        "    record = row.split('-')\n",
        "    #reading a record text to find which are empty text.\n",
        "    record[1]\n",
        "\n",
        "    assert(len(record[0])<120)\n",
        "  except:\n",
        "    unknown_publishers.append(index)"
      ],
      "metadata": {
        "id": "XV0dtpt_5xrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for unknown_publishers on the real news dataset to fill in with Unknown tag5\n",
        "unknown_publishers_dataSet2 = []\n",
        "for index, row in enumerate(train_real.text.values):\n",
        "  try:\n",
        "    record = row.split('-')\n",
        "    #reading a record text to find which are empty text.\n",
        "    record[1]\n",
        "\n",
        "    assert(len(record[0])<120)\n",
        "  except:\n",
        "    unknown_publishers_dataSet2.append(index)"
      ],
      "metadata": {
        "id": "wd5vNsqWjgHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unknown_publishers)"
      ],
      "metadata": {
        "id": "gs4DNGrynASr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unknown_publishers_dataSet2)"
      ],
      "metadata": {
        "id": "HEyjGE5unDTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real.iloc[unknown_publishers].text"
      ],
      "metadata": {
        "id": "yp8mDLDr7XZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_real.iloc[unknown_publishers_dataSet2]"
      ],
      "metadata": {
        "id": "8zYfiuzJnOVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real.iloc[8970]"
      ],
      "metadata": {
        "id": "BH2arify7b1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real = real.drop(8970, axis=0)"
      ],
      "metadata": {
        "id": "97euv2Vw72Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "publisher = []\n",
        "tmp_text = []\n",
        "\n",
        "for index, row in enumerate(real.text.values):\n",
        "  if index in unknown_publishers:\n",
        "    tmp_text.append(row)\n",
        "    publisher.append('Unknown')\n",
        "  else:\n",
        "    record = row.split('-', maxsplit=1)\n",
        "    publisher.append(record[0].strip())\n",
        "    tmp_text.append(record[1].strip())"
      ],
      "metadata": {
        "id": "Xz_YZHa-77ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real['publisher'] = publisher\n",
        "real['text'] = tmp_text"
      ],
      "metadata": {
        "id": "sZkk258l8p3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "publisher_ds2 = []\n",
        "tmp_text_ds2 = []\n",
        "\n",
        "for index, row in enumerate(train_real.text.values):\n",
        "  if index in unknown_publishers_dataSet2:\n",
        "    tmp_text_ds2.append(row)\n",
        "    publisher_ds2.append('Unknown')\n",
        "  else:\n",
        "    record = row.split('-', maxsplit=1)\n",
        "    publisher_ds2.append(record[0].strip())\n",
        "    tmp_text_ds2.append(record[1].strip())"
      ],
      "metadata": {
        "id": "uAS0jrhvoPsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_real['publisher'] = publisher_ds2\n",
        "train_real['text'] = tmp_text_ds2"
      ],
      "metadata": {
        "id": "lAKLdMEeoW8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking fake data for empty text in news:\n",
        "emtpy_fake_index = [index for index, text in enumerate(fake.text.tolist()) if str(text).strip()=='']"
      ],
      "metadata": {
        "id": "NbBnVvh8_1rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empty_train_fake_index = [index for index, text in enumerate(train_fake.text.tolist()) if str(text).strip()=='']"
      ],
      "metadata": {
        "id": "Af5Kz_vAo7K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake.iloc[emtpy_fake_index]"
      ],
      "metadata": {
        "id": "dQGyJmrkAgaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real['text'] = real['title'] + \" \" + real['text']\n",
        "fake['text'] = fake['title'] + \" \" + fake['text']"
      ],
      "metadata": {
        "id": "wE7bN2FZBWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real['text'] = real['text'].apply(lambda x: str(x).lower())\n",
        "fake['text'] = fake['text'].apply(lambda x: str(x).lower())"
      ],
      "metadata": {
        "id": "Nn3EYrJEC4O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real.head()"
      ],
      "metadata": {
        "id": "-BcvTq2PqeCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_real['text'] = train_real['text'].apply(lambda x: str(x).lower())\n",
        "train_fake['text'] = train_fake['text'].apply(lambda x: str(x).lower())"
      ],
      "metadata": {
        "id": "Bru4-PKkqZYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing text"
      ],
      "metadata": {
        "id": "gwiWmG2HDAe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real['class'] = 1\n",
        "fake['class'] = 0"
      ],
      "metadata": {
        "id": "X6IZWb1RDGUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real = real[['text', 'class']]"
      ],
      "metadata": {
        "id": "m6CVtSSLDNVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_real = train_real[['text', 'label']]\n",
        "train_real = train_real.rename(columns = {'label':'class', 'Text':'text'})"
      ],
      "metadata": {
        "id": "UlZ8a26xrkcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = fake[['text', 'class']]"
      ],
      "metadata": {
        "id": "_w3I-p-iDclM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fake = train_fake[['text', 'label']]\n",
        "train_fake = train_fake.rename(columns = {'label':'class', 'Text':'text'})"
      ],
      "metadata": {
        "id": "pZEYjHhfroOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([real, fake, train_real, train_fake], ignore_index=True)"
      ],
      "metadata": {
        "id": "8TN1zPU8DfxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "UWpVYBmajW_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(5)"
      ],
      "metadata": {
        "id": "zaQz5j7ADxCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.2.3\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install beautifulsoup4==4.9.1\n",
        "!pip install textblob==0.15.3\n",
        "!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall"
      ],
      "metadata": {
        "id": "-0Vr_DPBEcXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocess_kgptalkie as ps"
      ],
      "metadata": {
        "id": "JFBrGl5ME-TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].apply(lambda x: ps.remove_special_chars(x))"
      ],
      "metadata": {
        "id": "vbPM72--FAkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "z6R7kiigFG6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['class'].values"
      ],
      "metadata": {
        "id": "c2er6ddnGWSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [d.split() for d in data['text']. tolist()]"
      ],
      "metadata": {
        "id": "2gbr9sx-GauK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "D11MilDdkpRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 100\n",
        "w2v_model = gensim.models.Word2Vec(sentences = X, size = DIM, window = 10, min_count = 1)"
      ],
      "metadata": {
        "id": "uXwTzb9gHBhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)"
      ],
      "metadata": {
        "id": "ymVz-yAnIRgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#These sequence values represent a specific word\n",
        "#You can get the specific value for a word with tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "mJ1Qa_8GIj1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(x) for x in X], bins = 700)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kkutvUDzJLii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nos = np.array([len(x) for x in X])\n",
        "len(nos[nos>1000])"
      ],
      "metadata": {
        "id": "jJu9182uJoxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 1000\n",
        "X = pad_sequences(X, maxlen = maxlen)"
      ],
      "metadata": {
        "id": "km7KTf1XJ0Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X[101])"
      ],
      "metadata": {
        "id": "DALamw60J6B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "vocab = tokenizer.word_index "
      ],
      "metadata": {
        "id": "GNo13NUyKIVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what is a weight matrix for\n",
        "def get_weight_matrix(model):\n",
        "  weight_matrix = np.zeros((vocab_size, DIM))\n",
        "\n",
        "  for word, i in vocab.items():\n",
        "    weight_matrix[i] = model.wv[word]\n",
        "    \n",
        "    return weight_matrix"
      ],
      "metadata": {
        "id": "ypvbN5MNKcLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what is an embedding vector\n",
        "embedding_vectors = get_weight_matrix(w2v_model)"
      ],
      "metadata": {
        "id": "Cg0z3OYaLEe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors.shape"
      ],
      "metadata": {
        "id": "XIDQO_JTLW5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainableModel = Sequential()\n",
        "trainableModel.add(Embedding(vocab_size, output_dim = DIM, weights = [embedding_vectors], input_length = maxlen, trainable=True))\n",
        "trainableModel.add(Dropout(0.7))\n",
        "trainableModel.add(LSTM(units=128))\n",
        "trainableModel.add(Dropout(0.7))\n",
        "trainableModel.add(Dense(1, activation='sigmoid'))\n",
        "trainableModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "WJkT1Q_ELdiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonTrainableModel = Sequential()\n",
        "nonTrainableModel.add(Embedding(vocab_size, output_dim = DIM, weights = [embedding_vectors], input_length = maxlen, trainable = False))\n",
        "nonTrainableModel.add(LSTM(units=128))\n",
        "nonTrainableModel.add(Dense(1, activation='sigmoid'))\n",
        "nonTrainableModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "kT9YGvcLzWLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainableModel.summary()"
      ],
      "metadata": {
        "id": "x4Kjd_Knqr5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "metadata": {
        "id": "6sFjn6S-MxN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainableModel.fit(X_train, y_train, validation_split = 0.3, epochs = 6)"
      ],
      "metadata": {
        "id": "ZjIEHYY8M8Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonTrainableModel.fit(X_train, y_train, validation_split=0.3, epochs = 6)"
      ],
      "metadata": {
        "id": "qpc7Mlldzq3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_y_pred = (trainableModel.predict(X_test) >= 0.5).astype(int)\n",
        "nonTrainable_y_pred = (nonTrainableModel.predict(X_test) >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "D5uip6xvt7QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(trainable_y_pred, y_test)"
      ],
      "metadata": {
        "id": "f6f7GztKuaVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(nonTrainable_y_pred, y_test )"
      ],
      "metadata": {
        "id": "oAlGjzP00eaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, trainable_y_pred))"
      ],
      "metadata": {
        "id": "WKRmn0f8u3hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, nonTrainable_y_pred))\n"
      ],
      "metadata": {
        "id": "Otx3SFo22ure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainableModel.save('/content/trainableModel.h5')\n",
        "nonTrainableModel.save('/content/nonTrainableModel.h5')"
      ],
      "metadata": {
        "id": "X827iMCRveCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}